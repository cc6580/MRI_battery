{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"inversion grid search.py","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONSPG1xeK8HXEPdXaL8h7W"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oXhieCrV43ut","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n","# Imports and versions------------------------------------------------------------------------------------------------------------------\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import transforms\n","import matplotlib as mpl\n","# Define Agg as Backend for matplotlib when no X server is running\n","mpl.use('Agg') # AGG backend is for writing to file, not for rendering in a window.\n","from scipy import ndimage\n","from scipy.io import loadmat\n","from scipy import stats \n","import random\n","import time\n","import json\n","import os\n","import nibabel as nib\n","import pickle as pk\n","from scipy import interpolate\n","\n","#either choose keras or tf.keras, and make all imports from that package, and never mix it with the other bc they are not compatible.\n","#make sure you installed tensorflow-gpu as your tensorflow library in your current conda environment\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import models, layers\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","np.random.seed(30)\n","\n","#on HPC, we can check whether we have GPU support here---------------------------------------------------------------------------------------------\n","assert tf.config.list_physical_devices('GPU')\n","assert tf.test.is_built_with_cuda()\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","    raise SystemError('GPU device not found')\n","print(30*'-')\n","print('|','Found GPU at: {}'.format(device_name), '|')\n","print(30*'-')\n","\n","# defining 'pseudo' magnetometry mag-field readings grid compatible with real battery data readings-------------------------------------------------\n","rscale=np.array([7.40000000e-05, 1.97920635e-03, 3.88441270e-03, 5.78961905e-03,\n","       7.69482540e-03, 9.60003175e-03, 1.15052381e-02, 1.34104444e-02,\n","       1.53156508e-02, 1.72208571e-02, 1.91260635e-02, 2.10312698e-02,\n","       2.29364762e-02, 2.48416825e-02, 2.67468889e-02, 2.86520952e-02,\n","       3.05573016e-02, 3.24625079e-02, 3.43677143e-02, 3.62729206e-02,\n","       3.81781270e-02, 4.00833333e-02, 4.19885397e-02, 4.38937460e-02,\n","       4.57989524e-02, 4.77041587e-02, 4.96093651e-02, 5.15145714e-02,\n","       5.34197778e-02, 5.53249841e-02, 5.72301905e-02, 5.91353968e-02,\n","       6.10406032e-02, 6.29458095e-02, 6.48510159e-02, 6.67562222e-02,\n","       6.86614286e-02, 7.05666349e-02, 7.24718413e-02, 7.43770476e-02,\n","       7.62822540e-02, 7.81874603e-02, 8.00926667e-02, 8.19978730e-02,\n","       8.39030794e-02, 8.58082857e-02, 8.77134921e-02, 8.96186984e-02,\n","       9.15239048e-02, 9.34291111e-02, 9.53343175e-02, 9.72395238e-02,\n","       9.91447302e-02, 1.01049937e-01, 1.02955143e-01, 1.04860349e-01,\n","       1.06765556e-01, 1.08670762e-01, 1.10575968e-01, 1.12481175e-01,\n","       1.14386381e-01, 1.16291587e-01, 1.18196794e-01, 1.20102000e-01])\n","cscale=np.array([0.001     , 0.00229032, 0.00358065, 0.00487097, 0.00616129,\n","       0.00745161, 0.00874194, 0.01003226, 0.01132258, 0.0126129 ,\n","       0.01390323, 0.01519355, 0.01648387, 0.01777419, 0.01906452,\n","       0.02035484, 0.02164516, 0.02293548, 0.02422581, 0.02551613,\n","       0.02680645, 0.02809677, 0.0293871 , 0.03067742, 0.03196774,\n","       0.03325806, 0.03454839, 0.03583871, 0.03712903, 0.03841935,\n","       0.03970968, 0.041     ])\n","\n","# set up the grid for mag-susceptibility grid--------------------------------------------------------------------------------------------------------\n","battery_dims=np.array([5,30,40])*1e-3;\n","cell_dims=np.array([5,50,60])*1e-3;\n","probe_dist=1.59e-4; \n","npts=[1,16,32];\n","dV=np.prod(cell_dims/npts); \n","\n","# re-adjust for centering-----------------------------------------------------------------------------------------------------------------------------\n","# for first data\n","centery=0.021;\n","centerz=0.06;\n","\n","# for second data\n","centery=0.015;\n","centerz=0.077;\n","\n","# for damaged cell data\n","centery=0.020;\n","centerz=0.065;\n","\n","# for new send data\n","centery=0.021;\n","centerz=0.062;\n","\n","# covert both the mag-field grid and mag-susceptibility field into row-point representation-----------------------------------------------------------\n","\n","# for mag-suscep\n","srcpos=[[],[],[]]\n","for i in range(3):\n","    srcpos[i]=np.linspace(0,cell_dims[i],npts[i]+2)\n","    srcpos[i]=srcpos[i][1:(npts[i]+1)]\n","# recenter all y and z coordinates\n","srcpos[1]=srcpos[1]+centery-cell_dims[1]/2\n","srcpos[2]=srcpos[2]+centerz-cell_dims[2]/2\n","\n","srcxv,srcyv,srczv=np.meshgrid(srcpos[0],srcpos[1],srcpos[2],indexing='ij')\n","\n","src_fulllength=np.prod(npts)\n","src_pos_list=np.concatenate((srcxv.reshape((src_fulllength,1)),srcyv.reshape((src_fulllength,1)),srczv.reshape((src_fulllength,1))),axis=1)\n","\n","# for mag-field also insert the probe_distance\n","rv, cv = np.meshgrid(rscale, cscale, indexing='ij')\n","fulllength=np.prod(rv.shape)\n","field_pos_list=np.concatenate((cv.reshape((fulllength,1)),rv.reshape((fulllength,1))),axis=1)\n","field_pos_list=np.insert(field_pos_list,0,probe_dist,axis=1)\n","\n","# set up the conversion matrix between mag-suscep and mag-field---------------------------------------------------------------------------------------\n","oneD=0   # to do z-only calc in this framework for magnetic susceptibility (may be more stable)\n","# make sure to reshape such that multiply the correct field components\n","# A matrix is not very sparse, so maybe faster to do in non-sparse setup\n","fpl=field_pos_list.shape[0]\n","A=np.zeros((2,field_pos_list.shape[0],src_pos_list.shape[0],3),dtype=float)\n","for i in range(src_pos_list.shape[0]):\n","  # for every point in the source position, aka cell grid \n","    posdiff=src_pos_list[i,:]-field_pos_list\n","      # this is a broadcasted operation\n","      # posdiff is field_pos_list but every row is subtracted from ths ith row in src_pos_list\n","    inv_r=1/np.sqrt(np.sum(posdiff**2,axis=1))\n","      # performed for each row\n","    inv_r5=inv_r**5\n","    inv_r3=inv_r**3\n","    \n","    for fidx in range(2):\n","        fidx2=fidx+1    # this is the real dim index (compatible with sidx)\n","                        # since I only have y and z components of the field\n","        \n","        if oneD:\n","            sidx=2\n","            A[fidx,:,i,sidx]=3*posdiff[:,fidx2]*posdiff[:,sidx]*inv_r5\n","            # A[0, :, i, 2] = 3*posdiff[:,1]*posdiff[:,2] * inv_r5\n","            # A[1, :, i, 2] = 3*posdiff[:,2]*posdiff[:,2] * inv_r5\n","            if sidx==fidx2:\n","                A[fidx,:,i,sidx]=A[fidx,:,i,sidx]-inv_r3\n","                #A[1, :, i, 2] = A[1, :, i, 2] - inv_r3\n","        else:\n","            for sidx in range(3):\n","                A[fidx,:,i,sidx]=3*posdiff[:,fidx2]*posdiff[:,sidx]*inv_r5\n","                #A[0, :, i, 0] = 3*posdiff[:, 1] * posdiff[:,0] * inv_r5\n","                #A[0, :, i, 1] = 3*posdiff[:, 1] * posdiff[:,1] * inv_r5\n","                #A[0, :, i, 2] = 3*posdiff[:, 1] * posdiff[:,2] * inv_r5\n","                #A[1, :, i, 0] = 3*posdiff[:, 2] * posdiff[:,0] * inv_r5\n","                #A[1, :, i, 1] = 3*posdiff[:, 2] * posdiff[:,1] * inv_r5\n","                #A[1, :, i, 2] = 3*posdiff[:, 2] * posdiff[:,2] * inv_r5\n","                if sidx==fidx2:\n","                    A[fidx,:,i,sidx]=A[fidx,:,i,sidx]-inv_r3\n","                    #A[0, :, i, 1] = A[0, :, i, 1] - inv_r3\n","                    #A[1, :, i, 2] = A[1, :, i, 2] - inv_r3\n","    \n","A=A.reshape((fpl*2,src_pos_list.shape[0]*3))\n","\n","# proper conversion units\n","B0=20e-6\n","A=A*dV*B0/4/np.pi\n","\n","# generate psudo battery data sets -------------------------------------------------------------------------------------------------------------------\n","\n","# number of samples to be generated in the set\n","num_sim2=10000\n","\n","# define function for forwardly calculating the mag-field from mag-susceptibility\n","def calcfield(suscept):\n","  '''\n","  input:\n","    suscept: a 3-D array of magnetic susceptibility, must have the same shape as npts\n","  return:\n","    fieldy: the y-component of magnetic field vector space\n","    fieldz: the z-component of magnetic field vector space\n","  '''\n","  source_vec=np.squeeze(suscept[:,:,:]).reshape((src_fulllength*3,1))\n","  magfield=np.dot(A,source_vec)\n","  fieldy = magfield[0:fpl,0].reshape((rv.shape[0],rv.shape[1]))\n","  fieldz = magfield[fpl:2*fpl,0].reshape((rv.shape[0],rv.shape[1]))\n","  return fieldy, fieldz\n","\n","# for network best to create the susceptibility in the transposed version\n","training_labels=np.zeros((num_sim2, npts[2], npts[1],3))   \n","training_data=np.zeros((num_sim2, rv.shape[0], rv.shape[1],2))\n","\n","# generate random magntic susceptibility distributions, based on a set of random gaussian peaks \n","maxlevelrange=200e-6\n","numberpeaks=10;\n","idx1=range(npts[2])\n","idx2=range(npts[1])\n","midx1,midx2=np.meshgrid(idx1,idx2,indexing='ij')\n","for ii in range(num_sim2):\n","  # for now just produce z susceptibility (easier for checking result?)\n","\n","  # here provide alternative training set  exp(-x^2/(2sigma^2))\n","  if True:\n","    for iii in range(numberpeaks):\n","        pos1=np.random.rand(1)*npts[2]\n","        pos2=np.random.rand(1)*npts[1]\n","        w1=np.random.rand(1)*npts[2]/5+1\n","        w2=np.random.rand(1)*npts[1]/5+1\n","        amp=np.random.rand(1)*maxlevelrange\n","        training_labels[ii, :, :,2]=training_labels[ii, :, :,2]+amp*np.exp(-((midx1-pos1)/w1)**2-((midx2-pos2)/w2)**2)\n","  else:\n","    # for network best to create the susceptibility in the transposed version\n","    #training_labels[ii, :, :,2] = maxlevelrange*np.random.rand(npts[1], npts[2])\n","    training_labels[ii, :, :,2] = maxlevelrange*np.random.rand(npts[2], npts[1])  # only z susceptibility for now \n","      \n","  training_data[ii, :, :,0],training_data[ii, :, :,1] = calcfield(training_labels[ii,:,:,:])\n","\n","# train, validation, and test split\n","\n","# a list of random indices thats 40% of the samples\n","val_test_idx = random.sample(range(num_sim2), int(0.4*num_sim2))\n","# get the remainder of the indices as indices for the train set \n","train_idx = np.setdiff1d(range(num_sim2),val_test_idx)\n","\n","val_data = training_data[val_test_idx[:int(0.4*num_sim2/2)], :, :, :]\n","val_labels = training_labels[val_test_idx[:int(0.4*num_sim2/2)], :, :, :]\n","\n","test_data = training_data[val_test_idx[int(0.4*num_sim2/2):], :, :, :]\n","test_labels = training_labels[val_test_idx[int(0.4*num_sim2/2):], :, :, :]\n","\n","train_data = training_data[train_idx, :, :, :]\n","train_labels = training_labels[train_idx, :, :, :]\n","\n","#transformation into tensor\n","train_images_t2b=tf.constant(train_data)\n","train_labels_t2b=tf.constant(train_labels)\n","\n","val_images_t2b=tf.constant(val_data)\n","val_labels_t2b=tf.constant(val_labels)\n","\n","test_images_t2b=tf.constant(test_data)\n","test_labels_t2b=tf.constant(test_labels)\n","\n","def imshow_center(data):\n","  '''\n","  Display data as an image; i.e. on a 2D regular raster.\n","  input:\n","    data: a single battery's \n","      magnetic field image that is an array with shape [64,32] with scalar data,\n","      or magnetic susceptibility image that is an array with shape [32,16]\n","  output:\n","    figure\n","  '''\n","  maxval=np.max(np.abs(data))\n","  plt.imshow(data, cmap=\"seismic\",vmin=-maxval,vmax=maxval)\n","  plt.colorbar()\n","\n","imshow_center(train_images_t2b[33,:,:,0])\n","plt.savefig('training_sample_33.png')\n","\n","# for historical reasons, the magnetic susceptibility map shows up transpose, \n","# I guess it would be good to change that at some point, but for now keeping it \n","imshow_center(np.transpose(train_labels_t2b[33,:,:,2]))\n","plt.savefig('training_label_33.png')\n","\n","# model setup ----------------------------------------------------------------------------------------------------------------------------------------\n","\n","def get_figure():\n","    \"\"\"\n","    Returns:\n","      an emtpy figure and an empty axis objects to plot on. \n","    \n","    Removes top and right border and ticks, because those are ugly\n","    \"\"\"\n","    fig, ax = plt.subplots(1)\n","    plt.tick_params(top=False, right=False, which='both') \n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","    return fig, ax\n","\n","def conv_block_h2(input_tensor, num_filters, dropout_level = 0.15):\n","    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n","    encoder = layers.BatchNormalization()(encoder)\n","    encoder = layers.Activation('relu')(encoder)\n","    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n","    encoder = layers.BatchNormalization()(encoder)\n","    encoder = layers.Dropout(dropout_level)(encoder)\n","    encoder = layers.Activation('relu')(encoder)\n","    return encoder\n","\n","def encoder_block_h2(input_tensor, num_filters, dropout_level = 0.15):\n","    encoder = layers.BatchNormalization()(input_tensor) # added a first normalization layer\n","    encoder = conv_block_h2(input_tensor, num_filters)\n","    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n","    return encoder_pool, encoder\n","\n","def decoder_block_h2(input_tensor, concat_tensor, num_filters, dropout_level = 0.15):\n","    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n","    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n","    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","    decoder = layers.BatchNormalization()(decoder)\n","    decoder = layers.Dropout(dropout_level)(decoder)\n","    decoder = layers.Activation('relu')(decoder)\n","    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","    decoder = layers.BatchNormalization()(decoder)\n","    decoder = layers.Activation('relu')(decoder)\n","    return decoder\n","\n","def custom_loss_mse(y_true,y_pred):\n","    loss=K.mean(K.square(y_pred-y_true),axis=None)  #+K.sum(0*K.abs(penalty)) #can adjust the penalty weight\n","    return loss\n","\n","def custom_loss_rmse(y_true,y_pred):\n","    loss=K.sqrt(K.mean(K.square(y_pred-y_true),axis=None))  #+K.sum(0*K.abs(penalty)) #can adjust the penalty weight\n","    return loss\n","\n","def custom_loss_abs(y_true,y_pred):\n","    loss=K.mean(K.abs(y_pred-y_true),axis=None)  #+K.sum(0*K.abs(penalty)) #can adjust the penalty weight\n","    return loss\n","\n","# grid search over model ------------------------------------------------------------------------------------------------------------------------------\n","\n","# our model might not be able to be wrapped in a scikit_learn model\n","# so we have to do the for-loops by hand\n","# train and val sets are not part of the input, so they must be defined as global variables prior\n","\n","def grid_search(dropout_level_lst=[0.10], beta2_lst=[0.999], beta1_lst=[0.9], lr_rate_lst = [0.001],\n","                epsilon_lst=[1e-08], epo_lst = [10], bat_size_lst = [10]):\n","  # initialize values\n","  val_loss = np.inf\n","  best_params = dict()\n","\n","  for dp_level in dropout_level_lst:\n","    inputs_h2 = layers.Input(shape=(64,32,2))\n","    # same as (rscale, cscale, 2)\n","    # also same as the dimension for EACH image in the training set\n","    encoder0_pool_h2, encoder0_h2 = encoder_block_h2(inputs_h2, 8, dropout_level=dp_level)\n","    encoder1_pool_h2, encoder1_h2 = encoder_block_h2(encoder0_pool_h2, 16, dropout_level=dp_level)\n","    encoder2_pool_h2, encoder2_h2 = encoder_block_h2(encoder1_pool_h2, 32, dropout_level=dp_level)\n","    encoder3_pool_h2, encoder3_h2 = encoder_block_h2(encoder2_pool_h2, 64, dropout_level=dp_level)\n","    center_h2 = conv_block_h2(encoder3_pool_h2, 128, dropout_level=dp_level)\n","    decoder3_h2 = decoder_block_h2(center_h2, encoder3_h2, 64, dropout_level=dp_level)\n","    decoder2_h2 = decoder_block_h2(decoder3_h2, encoder2_h2, 32, dropout_level=dp_level)\n","    decoder1_h2 = decoder_block_h2(decoder2_h2, encoder1_h2, 16, dropout_level=dp_level)\n","    outputs_h2 = layers.Conv2D(3, (1, 1), padding=\"same\")(decoder1_h2)   # simply set number of output channels here, seems legit\n","\n","    model_ht2b = models.Model(inputs=[inputs_h2], outputs=[outputs_h2])\n","\n","    for beta2 in beta2_lst:\n","      for beta1 in beta1_lst:\n","        for lr_rate in lr_rate_lst:\n","          for eps in epsilon_lst:\n","            adam=keras.optimizers.Adam(learning_rate = lr_rate, beta_1 = beta1, beta_2=beta2, epsilon = eps)\n","\n","            model_ht2b.compile(optimizer=adam,\n","                               loss=custom_loss_rmse) # let's use rmse for optimization becuase it is a bigger target than mse\n","\n","            # construct checkpoint for saving the best model for current training\n","            filepath=\"current.best.h5\"\n","            checkpoint = ModelCheckpoint(filepath, \n","                                        monitor='val_loss', # this must be the same string as a metric from your model training verbose output\n","                                        verbose=1, \n","                                        save_best_only=True, \n","                                        mode='min', # we want minimum loss\n","                                        save_weights_only=False # we want to save the entire model, not just the weights\n","                                        )\n","            callbacks_list = [checkpoint]\n","\n","            for epo in epo_lst:\n","              for bat_size in bat_size_lst:\n","                start = time.time()\n","                history_ht2b = model_ht2b.fit(train_images_t2b, \n","                                              train_labels_t2b,\n","                                              validation_data = (val_images_t2b, val_labels_t2b),  \n","                                              epochs=epo, \n","                                              batch_size=bat_size, \n","                                              shuffle=True,\n","                                              callbacks = callbacks_list,\n","                                              verbose=1)\n","                training_time = time.time()-start\n","                \n","                # load best model from current training b/c the best model might not be the last model\n","                model_ht2b = tf.keras.models.load_model('current.best.h5',custom_objects={'custom_loss_rmse': custom_loss_rmse})\n","                new_loss = custom_loss_rmse(val_labels_t2b, model_ht2b.predict(val_images_t2b))\n","                \n","                if new_loss.numpy() < val_loss:\n","                  print()\n","                  print('final validation loss decreased from ', val_loss, ' to ', new_loss.numpy())\n","                  print('saving the current best model as the overall best model')\n","                  print(100*'*')\n","                  val_loss = new_loss.numpy()\n","                  \n","                  best_params['best_dropout_rate'] = dp_level\n","                  best_params['best_beta_2'] = beta2\n","                  best_params['best_beta_1'] = beta1\n","                  best_params['best_learning_rate'] = lr_rate\n","                  best_params['best_epsilon'] = eps\n","                  best_params['best_epochs'] = epo\n","                  best_params['best_batch_size'] = bat_size\n","\n","                  best_params['best_val_loss_reached'] = val_loss\n","                  best_params['training_time'] = training_time\n","                  # best_params['val_loss_his'] = history_ht2b.history['val_loss']\n","                  # best_params['train_loss_his'] = history_ht2b.history['loss']\n","                    # comment these out for now because they take way too much space when printed out \n","                  \n","                  # save the best overall grid-searched model found so far \n","                  model_ht2b.save('model.best.h5')\n","                  \n","                  # save history of validation-loss from the best model to observe epochs effect\n","                  with open('best_val_loss_history.db', 'wb') as file_pi:\n","                    pk.dump(history_ht2b.history['val_loss'], file_pi)\n","                  # later open with \n","                  # val_loss_history_ht2b = pk.load(open('best_val_loss_history.db', \"rb\"))\n","\n","                  # save history of training-loss from the best model to observe epochs effect\n","                  with open('best_train_loss_history.db', 'wb') as file_pi:\n","                    pk.dump(history_ht2b.history['loss'], file_pi)\n","                  # later open with \n","                  # train_loss_history_ht2b = pk.load(open('best_train_loss_history.db', \"rb\"))\n","\n","                  # save the best_params dictionary along the way incase training gets killed mid-way and the function doesn't get to finish\n","                  # \"w\" mode automatically overwrites if the file already exists\n","                  param_json = json.dumps(best_params)\n","                  f = open(\"best_params.json\",\"w\")\n","                  f.write(param_json)\n","                  f.close()\n","\n","                  # save a plot of the val_loss_history for the best performing model for observation\n","                  fig, ax = get_figure()\n","                  fig.set_size_inches(20,10)\n","                  num_epochs=len(history_ht2b.history['val_loss'])\n","                  startpoints=0\n","                  ax.set_yscale('log') # set y-axis to log_10 scale for better viewing\n","                  ax.plot((np.arange(num_epochs*1)+1)[startpoints:], \n","                          history_ht2b.history['loss'][startpoints:], \n","                          linewidth=1, color=\"orange\", \n","                          label=\"training_loss\")\n","                  ax.plot((np.arange(num_epochs*1)+1)[startpoints:], \n","                          history_ht2b.history['val_loss'][startpoints:], \n","                          linewidth=1, color=\"blue\", \n","                          label=\"validation loss\")\n","                  ax.set_xlabel('epochs')\n","                  ax.set_ylabel('log loss')\n","                  ax.legend(frameon=False);\n","                  fig.savefig('best_model_loss_history.png')\n","                else:\n","                  print('final validation loss did not decrease for this set of parameters')\n","                  print('current overall best model and parameters does not get updated')\n","                  print(100*'*')\n","  return best_params\n","\n","\n","best_params = grid_search(dropout_level_lst=[0.05, 0.10, 0.15], \n","                          beta2_lst=[0.999], \n","                          beta1_lst=[0.9], \n","                          lr_rate_lst = [0.001], # try 0.001 is pt1 and 0.01 is pt2  \n","                          epsilon_lst=[1e-08], \n","                          epo_lst = [3000], # we have 10,000 samples, also try 10000, 5000, 3000, (1000 does not work)\n","                          bat_size_lst = [i*64 for i in range(4,0,-1)] # we always want to start big and get smaller because only the first best val_loss model gets saved, and we want that to be the most efficient if several parameters equally good loss eventually\n","                          )\n","print(100*'*')\n","print(\"The best parameters are:\")\n","print(best_params)\n","print(100*'*')\n","\n","# save the final best_params dictionary\n","# bc earlier versions of python does not support 'with' statement we have to do this?\n","param_json = json.dumps(best_params)\n","f = open(\"best_params.json\",\"w\")\n","f.write(param_json)\n","f.close()\n","\n","# load the json file later with\n","# f = open(\"best_params.json\", \"r\")\n","# try:\n","#     best_params = json.load(f)\n","# finally:\n","#     f.close()\n","\n","\n","# predict using the overall best model ----------------------------------------------------------------------------------------------------------\n","\n","model_ht2b=tf.keras.models.load_model('model.best.h5',custom_objects={'custom_loss_rmse': custom_loss_rmse})\n","\n","susax=2\n","X_test = test_images_t2b\n","y_pred_ht2 = model_ht2b.predict(test_images_t2b)\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[20,:,:,susax])\n","plt.title(\"predicted susceptibility for the 20th battery in test set\")\n","plt.savefig(\"test_20_pred.png\")\n","\n","plt.figure()\n","imshow_center(test_labels_t2b[20,:,:,susax])\n","plt.title(\"actual susceptibility for the 20th battery in test set\")\n","plt.savefig(\"test_20_true.png\")\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[20,:,:,susax]-test_labels_t2b[20,:,:,susax])\n","# plot the the predicted susceptibility - actual susceptibility in the same chanel which is '2' for the 20th battery in test set\n","# we want this difference to be as small as possible, not too extreme in either way\n","plt.title(\"error between predicted and true susceptibility\")\n","plt.savefig('test_20_error.png')\n","\n","test_diff = test_labels_t2b - y_pred_ht2\n","test_diff = tf.keras.backend.flatten(test_diff)\n","plt.figure(figsize=(15,7))\n","plt.hist(test_diff, bins=30, density=True, label='difference');\n","mn, mx = plt.xlim()\n","plt.xlim(mn, mx)\n","kde_xs = np.linspace(mn, mx, 60)\n","kde = stats.gaussian_kde(test_diff)\n","plt.plot(kde_xs, kde.pdf(kde_xs), label=\"PDF\")\n","plt.legend(loc=\"upper left\")\n","plt.ylabel('Probability')\n","plt.xlabel('difference')\n","range = np.max([np.abs(np.min(test_diff)), np.abs(np.max(test_diff))])\n","plt.xlim(-range, range)\n","plt.title(\"Histogram of differences\");\n","plt.savefig(\"test_diff_hist.png\")\n","\n","yf,zf=calcfield(y_pred_ht2[20,:,:,:])\n","# calculate the y(dimension 0) and z(dimension 1) component of the magnetic field for the 20th battery\n","# from the predicted susceptibility output\n","# so we are kind of like recreating the input to see if it is the same as the actual input\n","# to see if the model learned the dipole forward kernel well\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[20,:,:,0])-yf)\n","plt.title(\"error between forwardly solved y-field from prediction and true input y-field\")\n","plt.savefig('test_20_yfield_diff.png')\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[20,:,:,1])-zf)\n","plt.title(\"error between forwardly solved z-field from prediction and true input z-field\")\n","plt.savefig('test_20_zfield_diff.png')\n","\n","\n","# calculate final test loss --------------------------------------------------------------------------------------------------------------------\n","\n","final_loss = custom_loss_rmse(test_labels_t2b, y_pred_ht2)\n","print('final RMSE loss on test set:', final_loss.numpy())\n","NRMSE = final_loss/K.mean(test_labels_t2b)\n","print('final normalized RMSE loss (div mean) on the test set:', NRMSE.numpy())\n","RMSE_range = final_loss /(tf.reduce_max(test_labels_t2b) - tf.reduce_min(test_labels_t2b))\n","print('final normalized RMSE loss (div range) on the test set:', RMSE_range.numpy())\n","test_arr = tf.keras.backend.flatten(test_labels_t2b).numpy()\n","IQR = stats.iqr(test_arr)\n","RMSE_IQR = final_loss/IQR\n","print('final normalized RMSE loss (div IQR) on the test set:', RMSE_IQR.numpy())\n","print('final norm of the difference tensor:', tf.norm(y_pred_ht2-test_labels_t2b).numpy())\n","Boll_NRMSE = tf.norm(y_pred_ht2-test_labels_t2b) / tf.norm(test_labels_t2b)\n","print('final Bollman normalized RMSE loss on the test set:', Boll_NRMSE.numpy())\n","\n","# view final loss per battery --------------------------------------------------------------------------------------------------------------------\n","Boll_NRMSE_test_lst = []\n","for i in np.arange(int(test_labels_t2b.shape[0])):\n","  # print(i)\n","  Boll_NRMSE = (tf.norm(y_pred_ht2[i,:,:,:]-test_labels_t2b[i,:,:,:]) \n","                / tf.norm(test_labels_t2b[i,:,:,:]))\n","  Boll_NRMSE_test_lst.append(Boll_NRMSE)\n","plt.figure(figsize=(15,7))\n","plt.plot(Boll_NRMSE_test_lst, label=\"Boll_NRMSE per battery\")\n","plt.legend()\n","plt.xlabel(\"battery #\")\n","plt.ylabel(\"Boll_NRMSE\")\n","plt.savefig(\"Boll_NRMSE_per_battery.png\")\n","\n","best_batt = np.argmin(Boll_NRMSE_test_lst)\n","best_batt_NRMSE = min(Boll_NRMSE_test_lst).numpy()\n","print(\"The best predicted battery is battery\", best_batt, \"from the test set with a Boll_NRMSE of \", best_batt_NRMSE)\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[best_batt,:,:,susax])\n","plt.title(\"predicted susceptibility for the \"+str(best_batt)+\" battery in test set\")\n","plt.savefig(\"test_best_pred.png\")\n","\n","plt.figure()\n","imshow_center(test_labels_t2b[best_batt,:,:,susax])\n","plt.title(\"actual susceptibility for the \"+str(best_batt)+\" battery in test set\")\n","plt.savefig(\"test_best_true.png\")\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[best_batt,:,:,susax]-test_labels_t2b[best_batt,:,:,susax])\n","# plot the the predicted susceptibility - actual susceptibility in the same chanel which is '2' for the best battery in test set\n","# we want this difference to be as small as possible, not too extreme in either way\n","plt.title(\"error between predicted and true susceptibility\")\n","plt.savefig('test_best_error.png')\n","\n","yf,zf=calcfield(y_pred_ht2[best_batt,:,:,:])\n","# calculate the y(dimension 0) and z(dimension 1) component of the magnetic field for the 20th battery\n","# from the predicted susceptibility output\n","# so we are kind of like recreating the input to see if it is the same as the actual input\n","# to see if the model learned the dipole forward kernel well\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[best_batt,:,:,0])-yf)\n","plt.title(\"error between forwardly solved y-field from prediction and true input y-field\")\n","plt.savefig('test_best_yfield_diff.png')\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[best_batt,:,:,1])-zf)\n","plt.title(\"error between forwardly solved z-field from prediction and true input z-field\")\n","plt.savefig('test_best_zfield_diff.png')\n","\n","# -------------------------------------------\n","\n","worst_batt = np.argmax(Boll_NRMSE_test_lst)\n","worst_batt_NRMSE = max(Boll_NRMSE_test_lst).numpy()\n","print(\"The worst predicted battery is battery\", worst_batt, \"from the test set with a Boll_NRMSE of \", worst_batt_NRMSE)\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[worst_batt,:,:,susax])\n","plt.title(\"predicted susceptibility for the \"+str(worst_batt)+\" battery in test set\")\n","plt.savefig(\"test_worst_pred.png\")\n","\n","plt.figure()\n","imshow_center(test_labels_t2b[worst_batt,:,:,susax])\n","plt.title(\"actual susceptibility for the \"+str(worst_batt)+\" battery in test set\")\n","plt.savefig(\"test_worst_true.png\")\n","\n","plt.figure()\n","imshow_center(y_pred_ht2[worst_batt,:,:,susax]-test_labels_t2b[worst_batt,:,:,susax])\n","# plot the the predicted susceptibility - actual susceptibility in the same chanel which is '2' for the best battery in test set\n","# we want this difference to be as small as possible, not too extreme in either way\n","plt.title(\"error between predicted and true susceptibility\")\n","plt.savefig('test_worst_error.png')\n","\n","yf,zf=calcfield(y_pred_ht2[worst_batt,:,:,:])\n","# calculate the y(dimension 0) and z(dimension 1) component of the magnetic field for the 20th battery\n","# from the predicted susceptibility output\n","# so we are kind of like recreating the input to see if it is the same as the actual input\n","# to see if the model learned the dipole forward kernel well\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[worst_batt,:,:,0])-yf)\n","plt.title(\"error between forwardly solved y-field from prediction and true input y-field\")\n","plt.savefig('test_worst_yfield_diff.png')\n","\n","plt.figure()\n","imshow_center(np.squeeze(X_test[worst_batt,:,:,1])-zf)\n","plt.title(\"error between forwardly solved z-field from prediction and true input z-field\")\n","plt.savefig('test_worst_zfield_diff.png')"],"execution_count":null,"outputs":[]}]}