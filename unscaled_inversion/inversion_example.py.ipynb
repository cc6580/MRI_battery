{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"inversion_example.py","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTA/Pkyca38WScr5lnov8S"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lBPXCeGFq0sc","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n","# Imports and versions\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import transforms\n","from scipy import ndimage\n","from scipy.io import loadmat\n","import nibabel as nib\n","import pickle as pk\n","from scipy import interpolate\n","\n","# set random seed for testing (to have the same state in subsequent tests, for production run don't do it)\n","np.random.seed(30)\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import models, layers\n","from tensorflow.keras import backend as K\n","\n","\n","\n","\"\"\"# Read battery magnetometry data\n","skipping this for now, since not needed immediately, but just defining the coordinate grid compatible with the battery data that we have.\n","\"\"\"\n","\n","# rscale and cscale as calculated from reading in real data and downsampling\n","# so that we do not need to read in experimental data right away\n","# check that can do without reading data\n","rscale=np.array([7.40000000e-05, 1.97920635e-03, 3.88441270e-03, 5.78961905e-03,\n","       7.69482540e-03, 9.60003175e-03, 1.15052381e-02, 1.34104444e-02,\n","       1.53156508e-02, 1.72208571e-02, 1.91260635e-02, 2.10312698e-02,\n","       2.29364762e-02, 2.48416825e-02, 2.67468889e-02, 2.86520952e-02,\n","       3.05573016e-02, 3.24625079e-02, 3.43677143e-02, 3.62729206e-02,\n","       3.81781270e-02, 4.00833333e-02, 4.19885397e-02, 4.38937460e-02,\n","       4.57989524e-02, 4.77041587e-02, 4.96093651e-02, 5.15145714e-02,\n","       5.34197778e-02, 5.53249841e-02, 5.72301905e-02, 5.91353968e-02,\n","       6.10406032e-02, 6.29458095e-02, 6.48510159e-02, 6.67562222e-02,\n","       6.86614286e-02, 7.05666349e-02, 7.24718413e-02, 7.43770476e-02,\n","       7.62822540e-02, 7.81874603e-02, 8.00926667e-02, 8.19978730e-02,\n","       8.39030794e-02, 8.58082857e-02, 8.77134921e-02, 8.96186984e-02,\n","       9.15239048e-02, 9.34291111e-02, 9.53343175e-02, 9.72395238e-02,\n","       9.91447302e-02, 1.01049937e-01, 1.02955143e-01, 1.04860349e-01,\n","       1.06765556e-01, 1.08670762e-01, 1.10575968e-01, 1.12481175e-01,\n","       1.14386381e-01, 1.16291587e-01, 1.18196794e-01, 1.20102000e-01])\n","cscale=np.array([0.001     , 0.00229032, 0.00358065, 0.00487097, 0.00616129,\n","       0.00745161, 0.00874194, 0.01003226, 0.01132258, 0.0126129 ,\n","       0.01390323, 0.01519355, 0.01648387, 0.01777419, 0.01906452,\n","       0.02035484, 0.02164516, 0.02293548, 0.02422581, 0.02551613,\n","       0.02680645, 0.02809677, 0.0293871 , 0.03067742, 0.03196774,\n","       0.03325806, 0.03454839, 0.03583871, 0.03712903, 0.03841935,\n","       0.03970968, 0.041     ])\n","\n","\n","\"\"\"# setting up the grid for the magnetic susceptibility\"\"\"\n","\n","# battery dimensions\n","battery_dims=np.array([5,30,40])*1e-3;\n","dims=np.array([5,50,60])*1e-3;  # cell dimensions\n","\n","\n","### probe_dist=2.3e-2;   # 2 cm top and bottom\n","probe_dist=1.59e-2;   # 2 cm top and bottom\n","\n","#probe_dist=1e-2;   # 2 cm top and bottom\n","\n","npts=[1,16,32];\n","\n","# volume element\n","# will this be accurate or is it minus 1?\n","dV=np.prod(dims/npts);   # this is volume per point in the susceptibility map, seems the correct way\n","\n","# some recentering of coordinates based on experimental data\n","# for first data\n","centery=0.021;\n","centerz=0.06;\n","\n","# for second data\n","centery=0.015;\n","centerz=0.077;\n","\n","# for damaged cell data\n","centery=0.020;\n","centerz=0.065;\n","\n","# for new send data\n","centery=0.021;\n","centerz=0.062;\n","\n","#field_dims=[dims(2)*3, dims(3)*3];\n","field_dims=np.array([60,80])*1e-3;\n","\n","\n","# see here: this volume probably calculated incorrectly, but it's also probably not needed\n","field_npts=[20,30];\n","dVfield=np.prod(field_dims/field_npts);\n","\n","\"\"\"### convert both the magnetic susceptibility grid positions and the magnetic field positions into lists `[x1,y1,z1; x2,y2,z2; etc ]`\n","`src_pos_list` is the list for magnetic susceptibility, and `field_pos_list` is the one for magnetic field.\n","\"\"\"\n","\n","# this way bottom of cell starts at x=0 (+padding), so the probe_dist is measured from the bottom of cell\n","srcpos=[[],[],[]]\n","for i in range(3):\n","    srcpos[i]=np.linspace(0,dims[i],npts[i]+2)\n","    srcpos[i]=srcpos[i][1:(npts[i]+1)]\n","srcpos[1]=srcpos[1]+centery-dims[1]/2\n","srcpos[2]=srcpos[2]+centerz-dims[2]/2\n","\n","srcxv,srcyv,srczv=np.meshgrid(srcpos[0],srcpos[1],srcpos[2],indexing='ij')\n","\n","src_fulllength=np.prod(npts)\n","src_pos_list=np.concatenate((srcxv.reshape((src_fulllength,1)),srcyv.reshape((src_fulllength,1)),srczv.reshape((src_fulllength,1))),axis=1)\n","\n","rv, cv = np.meshgrid(rscale, cscale, indexing='ij')  # ij indexing produces same shape as newy, newz\n","\n","# create field-pos / amp vectors\n","# remember that for two field components, I would have to stack them, so maybe best for now to keep them separate (position vs. field meas)\n","fulllength=np.prod(rv.shape)\n","# y, z, newy, newz\n","\n","field_pos_list=np.concatenate((cv.reshape((fulllength,1)),rv.reshape((fulllength,1))),axis=1)\n","field_pos_list=np.insert(field_pos_list,0,probe_dist,axis=1)\n","\n","\n","\"\"\"## set up conversion matrix between magn. susceptibility and magn. field\n","so that we have $\\mathbf{B}=A \\cdot \\mathbf{m}$ as a matrix-vector multiplication\n","\"\"\"\n","\n","# modified for multidim to include multiple field components (y-z)\n","# now modif for x-y-z source dims\n","\n","oneD=0   # to do z-only calc in this framework for magnetic susceptibility (may be more stable)\n","\n","# make sure to reshape such that multiply the correct field components\n","# A matrix is not very sparse, so maybe faster to do in non-sparse setup\n","fpl=field_pos_list.shape[0]\n","A=np.zeros((2,fpl,src_pos_list.shape[0],3),dtype=float)\n","for i in range(src_pos_list.shape[0]):\n","    posdiff=src_pos_list[i,:]-field_pos_list\n","      # this is a broadcasted operation\n","      # posdiff is field_pos_list but every row is subtracted from ths ith row in src_pos_list\n","    inv_r=1/np.sqrt(np.sum(posdiff**2,axis=1))\n","      # performed for each row\n","    inv_r5=inv_r**5\n","    inv_r3=inv_r**3\n","    \n","    for fidx in range(2):\n","        fidx2=fidx+1    # this is the real dim index (compatible with sidx)\n","                        # since I only have y and z components of the field\n","        \n","        if oneD:\n","            sidx=2\n","            A[fidx,:,i,sidx]=3*posdiff[:,fidx2]*posdiff[:,sidx]*inv_r5\n","            if sidx==fidx2:\n","                A[fidx,:,i,sidx]=A[fidx,:,i,sidx]-inv_r3\n","        else:\n","            for sidx in range(3):\n","                A[fidx,:,i,sidx]=3*posdiff[:,fidx2]*posdiff[:,sidx]*inv_r5\n","                if sidx==fidx2:\n","                    A[fidx,:,i,sidx]=A[fidx,:,i,sidx]-inv_r3\n","    \n","A=A.reshape((fpl*2,src_pos_list.shape[0]*3))\n","\n","\n","# proper conversion units\n","\n","B0=20e-6\n","A=A*dV*B0/4/np.pi\n","\n","\n","\"\"\"## left out SVD and pseudoinverse calculation here since not immediately relevant\n","\n","## Model setup\n","model based on papers (Bollman, etc) with some modifications, links given\n","\"\"\"\n","\n","# for regularization\n","dropout_level=0.15\n","\n","#adapted from https://colab.research.google.com/drive/1ltjXmi6fSAe4YBgJrmHH_wjTl9VxFRgl\n","\n","def get_figure():\n","    \"\"\"\n","    Returns figure and axis objects to plot on. \n","    Removes top and right border and ticks, because those are ugly\n","    \"\"\"\n","    fig, ax = plt.subplots(1)\n","    plt.tick_params(top=False, right=False, which='both') \n","    ax.spines['top'].set_visible(False)\n","    ax.spines['right'].set_visible(False)\n","    return fig, ax\n","\n","def conv_block_h2(input_tensor, num_filters):\n","    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n","    encoder = layers.BatchNormalization()(encoder)\n","    encoder = layers.Activation('relu')(encoder)\n","    encoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n","    encoder = layers.BatchNormalization()(encoder)\n","    encoder = layers.Dropout(dropout_level)(encoder)\n","    encoder = layers.Activation('relu')(encoder)\n","    return encoder\n","\n","def encoder_block_h2(input_tensor, num_filters):\n","    encoder = conv_block_h2(input_tensor, num_filters)\n","    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n","    return encoder_pool, encoder\n","\n","def decoder_block_h2(input_tensor, concat_tensor, num_filters):\n","    decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n","    decoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n","    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","    decoder = layers.BatchNormalization()(decoder)\n","    decoder = layers.Dropout(dropout_level)(decoder)\n","    decoder = layers.Activation('relu')(decoder)\n","    decoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n","    decoder = layers.BatchNormalization()(decoder)\n","    decoder = layers.Activation('relu')(decoder)\n","    return decoder\n","\n","# define the loss function for TF, for now just differentce beetween predicted and true magnetic fields\n","def custom_loss(y_true,y_pred):\n","    print(\"y_pred shape:\",y_pred[0,:,:,0].shape)\n","    #penalty=batt_mask128(y_pred[0,:,:,0])\n","    #loss=K.mean(K.square(y_pred-y_true),axis=-1)+np.sum(0*np.abs(penalty)) #can adjust the penalty weight\n","    loss=K.mean(K.square(y_pred-y_true),axis=None)  #+K.sum(0*K.abs(penalty)) #can adjust the penalty weight\n","    return loss\n","\n","inputs_h2 = layers.Input(shape=(64,32,2))\n","encoder0_pool_h2, encoder0_h2 = encoder_block_h2(inputs_h2, 8)\n","encoder1_pool_h2, encoder1_h2 = encoder_block_h2(encoder0_pool_h2, 16)\n","encoder2_pool_h2, encoder2_h2 = encoder_block_h2(encoder1_pool_h2, 32)\n","encoder3_pool_h2, encoder3_h2 = encoder_block_h2(encoder2_pool_h2, 64)\n","center_h2 = conv_block_h2(encoder3_pool_h2, 128)\n","decoder3_h2 = decoder_block_h2(center_h2, encoder3_h2, 64)\n","decoder2_h2 = decoder_block_h2(decoder3_h2, encoder2_h2, 32)\n","decoder1_h2 = decoder_block_h2(decoder2_h2, encoder1_h2, 16)\n","# figure out how to downsample filter sizes, simple with conv, but do I need any activations? \n","# removing one block gives output of fact 2 smaller each side\n","# but what happens to concatenation, probably should still use it, maybe in the filter reduction?\n","# actually this concatenation may not be needed\n","# does concatenation need the same number of filters?\n","# maybe could even do without concatenation?\n","# decoder0_h2 = decoder_block_h2(decoder1_h2, encoder0_h2, 8)\n","outputs_h2 = layers.Conv2D(3, (1, 1), padding=\"same\")(decoder1_h2)   # simply set number of output channels here, seems legit\n","\n","model_ht2b = models.Model(inputs=[inputs_h2], outputs=[outputs_h2])\n","\n","adam=keras.optimizers.Adam(beta_2=0.99)\n","\n","model_ht2b.compile(optimizer=adam,\n","             loss=custom_loss)\n","\n","model_ht2b.summary()\n","\n","\"\"\"## Generating Training Set\n","generating fake magnetic susceptibility distributions and calculating the expected magnetic fields from this based on known A matrix\n","\"\"\"\n","\n","num_sim2=200 #can adjust, higher means slower but more accurate\n","\n","#training_labels=np.zeros((num_sim2, npts[1], npts[2],3))\n","############\n","# for network best to create the susceptibility in the transposed version\n","training_labels=np.zeros((num_sim2, npts[2], npts[1],3))   \n","training_data=np.zeros((num_sim2, rv.shape[0], rv.shape[1],2))\n","\n","def calcfield(suscept):\n","    source_vec=np.squeeze(suscept[:,:,:]).reshape((src_fulllength*3,1))\n","    magfield=np.dot(A,source_vec)\n","    fieldy = magfield[0:fpl,0].reshape((rv.shape[0],rv.shape[1]))\n","    fieldz = magfield[fpl:2*fpl,0].reshape((rv.shape[0],rv.shape[1]))\n","    return fieldy, fieldz\n","\n","# generate random magntic susceptibility distributions, based on a set of random gaussian peaks \n","maxlevelrange=200e-6\n","numberpeaks=10;\n","idx1=range(npts[2])\n","idx2=range(npts[1])\n","midx1,midx2=np.meshgrid(idx1,idx2,indexing='ij')\n","for ii in range(num_sim2):\n","    # for now just produce z susceptibility (easier for checking result?)\n","    ############\n","    \n","    # here provide alternative training set  exp(-x^2/(2sigma^2))\n","    if True:\n","        for iii in range(numberpeaks):\n","            pos1=np.random.rand(1)*npts[2]\n","            pos2=np.random.rand(1)*npts[1]\n","            w1=np.random.rand(1)*npts[2]/5+1\n","            w2=np.random.rand(1)*npts[1]/5+1\n","            amp=np.random.rand(1)*maxlevelrange\n","            training_labels[ii, :, :,2]=training_labels[ii, :, :,2]+amp*np.exp(-((midx1-pos1)/w1)**2-((midx2-pos2)/w2)**2)\n","    else:\n","        # for network best to create the susceptibility in the transposed version\n","        #training_labels[ii, :, :,2] = maxlevelrange*np.random.rand(npts[1], npts[2])\n","        training_labels[ii, :, :,2] = maxlevelrange*np.random.rand(npts[2], npts[1])  # only z susceptibility for now \n","        \n","    training_data[ii, :, :,0],training_data[ii, :, :,1] = calcfield(training_labels[ii,:,:,:])\n","\n","# TF requires this kind of transformation into tensor\n","train_images_t2b=tf.constant(training_data)\n","train_labels_t2b=tf.constant(training_labels)\n","\n","\n","def imshow_center(data):\n","    maxval=np.max(np.abs(data))\n","    plt.imshow(data, cmap=\"seismic\",vmin=-maxval,vmax=maxval)\n","    plt.colorbar()\n","\n","imshow_center(train_images_t2b[33,:,:,0])\n","plt.savefig('training_sample.png')\n","\n","# for historical reasons, the magnetic susceptibility map shows up transpose, \n","# I guess it would be good to change that at some point, but for now keeping it \n","imshow_center(np.transpose(train_labels_t2b[10,:,:,2]))\n","plt.savefig('label_sample.png')\n","\n","\"\"\"# Training the model\"\"\"\n","\n","# I read that using adam with learning rate 0.001 is good\n","num_epochs=10 #can adjust ultimately should be quite large\n","history_ht2b = model_ht2b.fit(train_images_t2b, train_labels_t2b,  epochs=num_epochs, batch_size=5, shuffle=True)\n","\n","# plot loss fn vs. epochs\n","\n","#adapted from https://colab.research.google.com/drive/1ltjXmi6fSAe4YBgJrmHH_wjTl9VxFRgl\n","\n","loss_history_ht2b = history_ht2b.history['loss']\n","\n","fig, ax = get_figure()\n","\n","startpoints=0\n","ax.plot((np.arange(num_epochs*1)+1)[startpoints:], loss_history_ht2b[startpoints:], marker=\"o\", linewidth=2, color=\"orange\", label=\"loss1\")\n","ax.set_xlabel('epoch')\n","ax.legend(frameon=False);\n","\n","# ___ epochs until convergence\n","\n","fig.savefig('history.png')\n","\n","# save in one step\n","model_ht2b.save('model.h5')\n","\n","# save history (if needed separately)\n","with open('train_history.db', 'wb') as file_pi:\n","        pk.dump(loss_history_ht2b, file_pi)\n","\n","\"\"\"# Loading Saved Model and predicting\"\"\"\n","\n","# just for testing that I can read in model and proceed from here\n","#del model_ht2b\n","#del history_ht2b\n","del loss_history_ht2b\n","\n","loss_history_ht2b = pk.load(open('train_history.db', \"rb\"))\n","fig, ax = get_figure()\n","num_epochs=len(loss_history_ht2b)\n","startpoints=1900\n","ax.plot((np.arange(num_epochs*1)+1)[startpoints:], loss_history_ht2b[startpoints:], marker=\"o\", linewidth=2, color=\"orange\", label=\"loss1\")\n","ax.set_xlabel('epoch')\n","ax.legend(frameon=False);\n","\n","# load in one step\n","# fixed with adding custom_loss function, in future, better to save model and weights separately?\n","model_ht2b=tf.keras.models.load_model('model.h5',custom_objects={'custom_loss': custom_loss})\n","\n","# do I need to recompile after loading?\n","model_ht2b.compile(optimizer=adam,\n","             loss=custom_loss)\n","\n","#Predicting the training data, adapted from https://colab.research.google.com/drive/1ltjXmi6fSAe4YBgJrmHH_wjTl9VxFRgl\n","\n","test_patch_nbr = 20\n","susax=2\n","X_test = train_images_t2b[np.newaxis,test_patch_nbr,:,:,:]     # why do I need to add new axis for prediction set?\n","y_pred_ht2 = model_ht2b.predict(X_test)\n","plt.figure()\n","imshow_center(y_pred_ht2[0,:,:,susax]-train_labels_t2b[test_patch_nbr,:,:,susax])\n","yf,zf=calcfield(y_pred_ht2[0,:,:,:])\n","# yf and zf end up too large\n","plt.figure()\n","imshow_center(np.squeeze(X_test[0,:,:,0])-yf)\n","plt.figure()\n","imshow_center(np.squeeze(X_test[0,:,:,1])-zf)\n","#plt.savefig('prediction.png')"],"execution_count":0,"outputs":[]}]}